{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human disease category indicators -- EMIP, etc.\n",
    "* This code integrates UniProt entries, with PICKLE PPI data, and Orphanet disease data to prepare human disease project analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "tiny = np.finfo(np.float32).tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Integrating PICKLE data with UniProt\n",
    "##### PICKLE PPI data TXT file from:\n",
    ">* http://www.pickle.gr/Data/2.5/PICKLE2_5_UniProtNormalizedTabular-default.zip\n",
    "##### Human proteome data TAB file from:\n",
    ">* https://www.uniprot.org/uniprot/?query=proteome:UP000005640&format=tab&force=true&columns=id,reviewed,genes(PREFERRED),protein%20names,sequence,database(Orphanet),comment(INVOLVEMENT%20IN%20DISEASE)&compress=yes\n",
    ">###### *** Downloaded files and the code should be in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_ints = pd.read_csv('UniProtNormalizedTabular-default.txt', delimiter='\\t', usecols=['InteractorA', 'InteractorB']) # import PICKLE data\n",
    "Human_proteome = pd.read_csv('uniprot-proteome UP000005640.tab', delimiter='\\t') # import human proteome data\n",
    "\n",
    "# Converting PICKLE binary interaction format to uniprot 'interacts with' format\n",
    "grp1 = pickle_ints.groupby('InteractorA')['InteractorB'].apply(list).apply(lambda x : '; '.join(str(elem) for elem in x)).reset_index(name='Interacts with').rename(columns={'InteractorA':'Entry'})\n",
    "grp2 = pickle_ints.groupby('InteractorB')['InteractorA'].apply(list).apply(lambda x : '; '.join(str(elem) for elem in x)).reset_index(name='Interacts with').rename(columns={'InteractorB':'Entry'})\n",
    "\n",
    "# Replacing interactions column of the PICKLE database\n",
    "ints_in_uniprot_format = grp1.append(grp2).groupby('Entry')['Interacts with'].apply(list).apply(lambda x : '; '.join(str(elem) for elem in x)).reset_index(name='Interacts with')\n",
    "ints_in_uniprot_format['Interacts with'] = ints_in_uniprot_format['Interacts with'].apply(lambda x : x.split('; ')).apply(lambda x : list(set(list(x)))).apply(lambda x : '; '.join(str(elem) for elem in x))\n",
    "Human_proteome = Human_proteome.merge(ints_in_uniprot_format, how='left', on='Entry')\n",
    "\n",
    "# Generating a file for human proteome data integrated with PICKLE interactions\n",
    "Human_proteome.to_csv('Human proteome with Pickle interactions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the generated CSV file and creating dictionaries and lists:\n",
    "* indx is a dictionary that will be used to find a given protein entry and return its index.\n",
    "* ints is the list of interaction lists (list of lists).\n",
    "* seqs is the list for sequence strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = {} # entry indexes dictionary\n",
    "ints = [] # list of interactions\n",
    "n_ints =[] # list of number of interactions\n",
    "seqs = [] # list of sequence strings\n",
    "Entry= [] # list of Entries\n",
    "prot_name = [] # list of protein names\n",
    "orpha_uniprot = [] # list of orphanet numbers\n",
    "involvement = [] # list of involvement in diseases\n",
    "status = [] # list of Sprot/trembl status\n",
    "gene_name = [] # list of Gene names (primary)\n",
    "\n",
    "with open('Human proteome with Pickle interactions.csv', newline = '') as csvfile:\n",
    "    n = 0 # number of row\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        indx[row['Entry']] = n\n",
    "        # create list of lists for interactions and appending other lists as needed:\n",
    "        ints.append([entry for entry in row['Interacts with'].split('; ')] if row['Interacts with'] is not '' else [])\n",
    "        n_ints.append(row['Interacts with'].count('; ') + 1 if row['Interacts with'] is not '' else 0)\n",
    "        seqs.append(row['Sequence'])\n",
    "        Entry.append(row['Entry'])\n",
    "        prot_name.append(row['Protein names'])\n",
    "        orpha_uniprot.append(row['Cross-reference (Orphanet)'])\n",
    "        involvement.append(row['Involvement in disease'])\n",
    "        status.append(row['Status'])\n",
    "        gene_name.append(row['Gene names  (primary )'])\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All alphabet letters are counted in sequence strings forming the rep_ent matrix whose rows are representative of protein entries and columns are representative of A-Z letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_ent = np.zeros((n, 22), dtype = np.float32) # matrix of number of repitition of amino acids in entry sequences initialized with zeros\n",
    "for i in range(n):\n",
    "    rep_ent[(i,  0)] = seqs[i].count('A') # count A string in Sequence\n",
    "    rep_ent[(i,  1)] = seqs[i].count('C') # count C string in Sequence\n",
    "    rep_ent[(i,  2)] = seqs[i].count('D') + seqs[i].count('B') / 2 # count D string in Sequence\n",
    "    #(Since B residue is either D or N, half of its frequency has been added here)\n",
    "    rep_ent[(i,  3)] = seqs[i].count('E') + seqs[i].count('Z') / 2 # count E string in Sequence\n",
    "    #(Since Z residue is either E or Q, half of its frequency has been added here)\n",
    "    rep_ent[(i,  4)] = seqs[i].count('F') # count F string in Sequence\n",
    "    rep_ent[(i,  5)] = seqs[i].count('G') # count G string in Sequence\n",
    "    rep_ent[(i,  6)] = seqs[i].count('H') # count H string in Sequence\n",
    "    rep_ent[(i,  7)] = seqs[i].count('I') + seqs[i].count('J') / 2 # count I string in Sequence\n",
    "    #(Since J residue is either I or L, half of its frequency has been added here)\n",
    "    rep_ent[(i, 8)] = seqs[i].count('K') # count K string in Sequence\n",
    "    rep_ent[(i, 9)] = seqs[i].count('L') + seqs[i].count('J') / 2 # count L string in Sequence\n",
    "    #(Since J residue is either I or L, half of its frequency has been added here)\n",
    "    rep_ent[(i, 10)] = seqs[i].count('M') # count M string in Sequence\n",
    "    rep_ent[(i, 11)] = seqs[i].count('N') + seqs[i].count('B') / 2 # count N string in Sequence\n",
    "    #(Since B residue is either D or N, half of its frequency has been added here)\n",
    "    rep_ent[(i, 12)] = seqs[i].count('O') # count O string in Sequence\n",
    "    rep_ent[(i, 13)] = seqs[i].count('P') # count P string in Sequence\n",
    "    rep_ent[(i, 14)] = seqs[i].count('Q') + seqs[i].count('Z') / 2 # count Q string in Sequence\n",
    "    #(Since Z residue is either E or Q, half of its frequency has been added here)\n",
    "    rep_ent[(i, 15)] = seqs[i].count('R') # count R string in Sequence\n",
    "    rep_ent[(i, 16)] = seqs[i].count('S') # count S string in Sequence\n",
    "    rep_ent[(i, 17)] = seqs[i].count('T') # count T string in Sequence\n",
    "    rep_ent[(i, 18)] = seqs[i].count('U') # count U string in Sequence\n",
    "    rep_ent[(i, 19)] = seqs[i].count('V') # count V string in Sequence\n",
    "    rep_ent[(i, 20)] = seqs[i].count('W') # count W string in Sequence\n",
    "    rep_ent[(i, 21)] = seqs[i].count('Y') # count Y string in Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating disease indicators: CAIR and EMIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ent = rep_ent @ np.ones((22, 1), dtype = np.float32) # the length of Entry sequences is calculated by ones-matrix multiplication\n",
    "P = rep_ent / (len_ent) # probability of each amino acid for Entries\n",
    "cair_ent = -np.sum(P * (np.log2(P + tiny) / np.log2(22)), axis = 1, keepdims = True) # calculating CAIRs of Entries (tiny is used to avoid log0)\n",
    "pi_int = np.zeros((n, 1), dtype = np.float32) # creating zeros matrix\n",
    "rep_net = np.copy(rep_ent) # hard copying rep_ent for rep_net (i.e. frequency of residues in a PPI network) calculation\n",
    "for i in range(n):\n",
    "    for entry in ints[i]:\n",
    "        try:\n",
    "            ind = indx[entry]\n",
    "            pi_int[(i, 0)] += cair_ent[(ind, 0)] * len_ent[(ind, 0)] # pi_int matrix is filled in by summation of interactors' PIs\n",
    "            rep_net[(i), 0:22] += rep_ent[(ind), 0:22] # rep_net is filled in by adding residue frequencies of interactors \n",
    "        except KeyError:\n",
    "            None\n",
    "len_net = rep_net @ np.ones((22, 1), dtype = np.float32) # calculating accumulative residue length of networks (by ones-matrix multiplication)\n",
    "P = rep_net / (len_net) # probability of each amino acid for networks\n",
    "cair_net = -np.sum(P * (np.log2(P + tiny) / np.log2(22)), axis = 1, keepdims = True) # calculating CAIRs of networks (tiny is used to avoid log0)\n",
    "net_inf = len_net * cair_net # network information\n",
    "emip = net_inf - pi_int # EMIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Integrating the data with Orphanet disease database and gene age data which are available open-source at:\n",
    "##### Orphanet database XML file from:\n",
    ">* http://www.orphadata.org/data/xml/en_product9_prev.xml\n",
    ">###### *** This file should be saved as a CSV format file using Excel.\n",
    "##### Gene age consensus article:\n",
    ">* Liebeskind BJ, McWhite CD, Marcotte EM. Towards consensus gene ages. Genome biology and evolution. 2016 Jun 1;8(6):1812-23.\n",
    ">* https://github.com/marcottelab/Gene-Ages/blob/master/Main/main_HUMAN.csv\n",
    "\n",
    ">###### *** Downloaded files and the code should be in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age = pd.read_csv('main_HUMAN.csv', usecols=['Entry', 'modeAge']) # adding gene ages\n",
    "\n",
    "## preparing and adding the Orphanet data as described in Methods section\n",
    "# preparing Orphanet raw (unfiltered) data\n",
    "orphadata_unprocessed = pd.read_csv('en_product9_prev.csv', encoding = \"ISO-8859-1\",\n",
    "                                    usecols=['OrphaNumber', 'ExpertLink', 'Name', 'Name4', 'Source', 'Name12', 'Name18', 'Name21',\n",
    "                                             'Name24']).rename(columns={'OrphaNumber': 'OrphaNumber (source: Orphanet)',\n",
    "                                                                        'Name': 'Disease name (source: Orphanet)', 'Name4': 'Disease type',\n",
    "                                                                        'Name12': 'Occurence type', 'Name18': 'Occurence value',\n",
    "                                                                        'Name21': 'Geo distrib', 'Name24': 'Validity'}) # reading columns renaming them\n",
    "orphadata_unprocessed['OrphaNumber (source: Orphanet)'] = orphadata_unprocessed['OrphaNumber (source: Orphanet)'].astype(str) # raw data of Orphanet\n",
    "\n",
    "# preparing Orphanet processed (filtered) data\n",
    "orphadata = pd.read_csv('en_product9_prev.csv', encoding = \"ISO-8859-1\",\n",
    "                        usecols=['OrphaNumber', 'ExpertLink', 'Name', 'Name4', 'Source', 'Name12', 'Name18', 'Name21',\n",
    "                                 'Name24']).rename(columns={'OrphaNumber': 'OrphaNumber (source: Orphanet)',\n",
    "                                                            'Name': 'Disease name (source: Orphanet)', 'Name4': 'Disease type',\n",
    "                                                            'Name12': 'Occurence type', 'Name18': 'Occurence value',\n",
    "                                                            'Name21': 'Geo distrib', 'Name24': 'Validity'}) # reading columns again renaming them \n",
    "# applying filters as explained in Methods section\n",
    "orphadata = orphadata[orphadata['Geo distrib'] == 'Worldwide'] # filtering out all locally distributed diseases -- Only keeps 'Worldwide'\n",
    "orphadata = orphadata[orphadata['Occurence value'] != 'Unknown'] # filtering out 'Unknown' occurences\n",
    "orphadata = orphadata[orphadata['Occurence value'] != 'Not yet documented'] # filtering out missing occurence values\n",
    "orphadata = orphadata[orphadata['Occurence type'] != 'Lifetime Prevalence'] # filtering out 'Lifetime Prevalence'\n",
    "orphadata = orphadata[orphadata['Occurence type'] != 'Cases/families'] # filtering out 'Cases/families'\n",
    "orphadata = orphadata.dropna(subset=['Occurence value']) # removing NAs in Occurence column\n",
    "orphadata.drop_duplicates('OrphaNumber (source: Orphanet)', keep='first', inplace=True) # preference is implied as below: (see methods section)\n",
    "                                                                                        # 1- Incidence, 2- Prevalence at birth, 3- Point prevalence\n",
    "orphadata['OrphaNumber (source: Orphanet)'] = orphadata['OrphaNumber (source: Orphanet)'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the first output (out1) file only designates the 'disease probability' according to the UniProt 'Involvement in disease' column\n",
    "prim_out = pd.DataFrame({'Status': status,\n",
    "                        'Entry': Entry,\n",
    "                        'Gene name': gene_name,\n",
    "                        'Protein name': prot_name,\n",
    "                        'Protein length': len_ent[:, 0],\n",
    "                        'CAIR (in pits)': cair_ent[:, 0],\n",
    "                        'Number of interactions': n_ints,\n",
    "                        'EMIP (in pits)': emip[:, 0],\n",
    "                        'Orpha no. (source: UniProt)': orpha_uniprot,\n",
    "                        'Involvement in disease (source: UniProt)': involvement}) # creating the primary output dataframe\n",
    "prim_out = prim_out.merge(Age, how='left', on='Entry').rename(columns={'modeAge': 'Gene age'}) # adding gene age column\n",
    "out1 = prim_out.copy() # hard-copying to write out1\n",
    "out1.to_csv('w%entries_w%diseases_w%uniprot_wo%orpha.csv', index = False) # writing the CSV out1\n",
    "\n",
    "## the second output (out2) file merges UNPROCESSED Orphadata separating the data with respect to Orpha numbers; represents all of the available disease data\n",
    "prim_out['Orpha no. (source: UniProt)'] = prim_out['Orpha no. (source: UniProt)'].apply(lambda x : x.split(';')[:-1]).apply(list) # preparing for the 'explode' function\n",
    "prim_out = prim_out.explode('Orpha no. (source: UniProt)') # requires pandas version 0.25 or later; separates the data with respect to Orpha numbers\n",
    "out2 = prim_out.merge(orphadata_unprocessed, how='left', left_on = 'Orpha no. (source: UniProt)', right_on = 'OrphaNumber (source: Orphanet)')\n",
    "out2 = out2.reindex(columns=['Orpha no. (source: UniProt)',\n",
    "                             'Involvement in disease (source: UniProt)', \n",
    "                             'OrphaNumber (source: Orphanet)',\n",
    "                             'Disease name (source: Orphanet)',\n",
    "                             'Disease type', \n",
    "                             'Occurence value',\n",
    "                             'Occurence type',\n",
    "                             'Geo distrib',\n",
    "                             'Validity',\n",
    "                             'Source',\n",
    "                             'Status',\n",
    "                             'Entry',\n",
    "                             'Gene name',\n",
    "                             'Protein name',\n",
    "                             'Protein length',\n",
    "                             'CAIR (in pits)',\n",
    "                             'Number of interactions',\n",
    "                             'EMIP (in pits)',\n",
    "                             'Gene age',\n",
    "                             'ExpertLink']) # reindexing columns\n",
    "out2['Orpha no. (source: UniProt)'] = out2['Orpha no. (source: UniProt)'].astype(float) # retyping Orpha numbers\n",
    "out2 = out2.sort_values(by=['Orpha no. (source: UniProt)']) # sorting based on Orpha numbers\n",
    "out2.to_csv('w%entries_w%diseases(expanded)_w%uniprot_w%orpha(unprocessed).csv', index = False) # writing the CSV output\n",
    "\n",
    "## the third output (out3) file merges PROCESSED Orphadata deleting all proteins w/o an Orpha number\n",
    "out3 = prim_out.merge(orphadata, how='left', left_on = 'Orpha no. (source: UniProt)', right_on = 'OrphaNumber (source: Orphanet)') # merges Orpha numbers from two sources\n",
    "out3 = out3.dropna(subset=['OrphaNumber (source: Orphanet)']) # dropping NAs\n",
    "out3 = out3.reindex(columns=['OrphaNumber (source: Orphanet)',\n",
    "                             'Disease name (source: Orphanet)',\n",
    "                             'Involvement in disease (source: UniProt)',\n",
    "                             'Disease type',\n",
    "                             'Occurence value',\n",
    "                             'Occurence type',\n",
    "                             'Geo distrib',\n",
    "                             'Validity',\n",
    "                             'Source',\n",
    "                             'Status',\n",
    "                             'Entry',\n",
    "                             'Gene name',\n",
    "                             'Protein name',\n",
    "                             'Protein length',\n",
    "                             'CAIR (in pits)',\n",
    "                             'Number of interactions',\n",
    "                             'EMIP (in pits)',\n",
    "                             'Gene age',\n",
    "                             'ExpertLink']) # reindexing columns\n",
    "out3.to_csv('w%entries(dropped)_w%diseases(expanded)_w%uniprot_w%orpha(processed).csv', index=False) # writing the CSV output\n",
    "\n",
    "## the forth output (out4) file merges PROCESSED Orphadata and calculates total occurence for all proteins; out4 is the final output\n",
    "out4 = prim_out.merge(orphadata, how='left', left_on = 'Orpha no. (source: UniProt)', right_on = 'OrphaNumber (source: Orphanet)') # merges Orpha numbers\n",
    "out4['Orpha no. (source: UniProt)'] = out4['Orpha no. (source: UniProt)'].fillna(0) # filling NAs with zeros to avoid unequality of NAs\n",
    "out4['OrphaNumber (source: Orphanet)'] = out4['OrphaNumber (source: Orphanet)'].fillna(0) # filling NAs with zeros to avoid unequality of NAs\n",
    "out4['Occurence value'] = out4.apply(lambda row : row['Occurence value'] if row['Orpha no. (source: UniProt)'] == row['OrphaNumber (source: Orphanet)'] else tiny, axis=1) # separating out NAs\n",
    "out4['Occurence value'] = out4['Occurence value'].fillna(0) # Giving zeros for the practicability of summation\n",
    "out4['Occurence value'] = out4['Occurence value'].replace({'<1 / 1 000 000' : 0.000001, '1-9 / 100 000': 0.00005, '1-9 / 1 000 000': 0.000005,\n",
    "                                                           '1-5 / 10 000': 0.0003, '>1 / 1000': 0.001, '6-9 / 10 000': 0.00075}) # quantifying the ranked Orpha data with means of the rank\n",
    "\n",
    "Tot_occur = out4[out4['Occurence value'] != 0].groupby('Entry')['Occurence value'].apply(sum).reset_index(name='Total occurence value') # summing up total occurences\n",
    "Tot_occur['Total occurence value'] = Tot_occur['Total occurence value'].apply(lambda x : 'NA' if x < 0.000000001 else x) # returning NA values back\n",
    "out4 = out1.merge(Tot_occur, how='left', on='Entry') # merging total occurences\n",
    "out4['Total occurence value'] = out4['Total occurence value'].fillna(0) # Giving zeros to non-diseases to distinguish it from real NAs\n",
    "out4 = out4.reindex(columns=['Status',\n",
    "                             'Entry',\n",
    "                             'Gene name',\n",
    "                             'Protein name',\n",
    "                             'Protein length',\n",
    "                             'CAIR (in pits)',\n",
    "                             'Number of interactions',\n",
    "                             'EMIP (in pits)',\n",
    "                             'Orpha no. (source: UniProt)',\n",
    "                             'Total occurence value',\n",
    "                             'Involvement in disease (source: UniProt)',\n",
    "                             'Gene age']) # reindexing columns\n",
    "out4.to_csv('w%entries_w%diseases(accumulated)_w%uniprot_w%orpha(processed).csv', index=False) # writing the CSV output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
